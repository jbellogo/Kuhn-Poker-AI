{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counterfactual Regret Minimization with Kuhn Poker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives an introduction to\n",
    "Counterfactual Regret Minimization using the game Kuhn Poker.\n",
    "\n",
    "If you have comments find me on [Twitter(@vpj)](https://twitter.com/vpj), or open an issue\n",
    "in the [Github repo](https://github.com/lab-ml/poker)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Game\n",
    "\n",
    "[Kuhn Poker](https://en.wikipedia.org/wiki/Kuhn_poker) is a two player\n",
    "3-card betting game.\n",
    "The players are dealt one card each out of Ace, King and Queen (no suits).\n",
    "There are only three cards in the pack so one card is left out.\n",
    "Ace beats King and Queen and King beats Queen - just like in normal ranking of cards.\n",
    "\n",
    "Both players ante $S$ chips (blindly bet $S$ chips).\n",
    "After looking at the cards,\n",
    "the first player can either pass or bet $1$ chip.\n",
    "* If first player passes, the the player with higher card wins the pot.\n",
    "* If first player bets, the second play can bet (i.e. call) $1$ chip or pass (i.e. fold).\n",
    "    * If the second player bets and the  player with the higher card wins the pot.\n",
    "    * If the second player passes (i.e. folds) the first player gets the pot.\n",
    "\n",
    "This game is played repeatedly and a good strategy will optimize for the\n",
    "long term utility (or winnings).\n",
    "\n",
    "Here's some example games:\n",
    "\n",
    "* `KAp` - Player 1 gets *K*. Player 2 gets *A*. Player 1 passes. Player 2 doesn't get a betting chance and Player 2 wins the pot of $2S$ chips.\n",
    "* `QKbp` - Player 1 gets *Q*. Player 2 gets *K*. Player 1 bets a chip. Player 2 passes (folds). Player 1 gets the pot of $2S+1$ because Player 2 folded.\n",
    "* `QAbb` - Player 1 gets *Q*. Player 2 gets *A*. Player 1 bets a chip. Player 2 also bets (calls). Player 2 wins the pot of $2S+2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nash Equilibrium\n",
    "\n",
    "By [Nash theorem](https://mathworld.wolfram.com/NashsTheorem.html)\n",
    "this game has a\n",
    "[Nash equilibrium](https://en.wikipedia.org/wiki/Nash_equilibrium).\n",
    "That is, there exists a combination strategies for the two players, such that\n",
    "neither of them can increase utility by changing only her strategy.\n",
    "\n",
    "Let's try to derive the Nash equilibrium mathematically,\n",
    "before trying Counter Factual Regret Minimization.\n",
    "\n",
    "1. If first player has an A he should always bet.\n",
    "2. If second player has an A he should bet (if first player did bet).\n",
    "3. If second player has a Q he should pass.\n",
    "4. If first player has a K, because of 1. and 2., he would only lose if he bets. So first player should pass if he has a K.\n",
    "5. If first player has a Q, he should sometimes bluff and bet. Lets say first player bets with probabiliy $p_1$ if he gets a Q.\n",
    "6. Similarly the second player will bet with probability $p_2$ if he gets a K (if first player did bet).\n",
    "\n",
    "Then the total utility for the first player for cards (AK, AQ, KA, KQ, QA, QK) is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "U_1 = \\frac{1}{6} \\big[ \n",
    "& + (1 + S) p_2 + S (1 - p_2) \\label{AK}\\tag{AK} \\\\\n",
    "& + S \\label{AQ}\\tag{AQ} \\\\\n",
    "& - S \\label{KA}\\tag{KA} \\\\\n",
    "& + S \\label{KQ}\\tag{KQ} \\\\\n",
    "& - (1 + S) p_1 - S (1 - p_1) \\label{QA}\\tag{QA} \\\\\n",
    "& - (1 + S) p_1 p_2 + S p_1 (1 - p_2) - S (1 - p_1)\n",
    "\\big] \\label{QK}\\tag{QK} \\\\\n",
    "= \\frac{1}{6} \\big[ \n",
    "& + S + p_2 \\\\\n",
    "& + S \\\\\n",
    "& - S - p_1 \\\\\n",
    "& - p_1 p_2 - 2 S p_1 p_2 + 2 S p_1 - S\n",
    "\\big] \\\\\n",
    "= \\frac{1}{6} \\big[ \n",
    "& + p_2 - p_1 - p_1 p_2 - 2 S p_1 p_2 + 2 S p_1\n",
    "\\big] \\\\\n",
    "= \\frac{1}{6} \\big[ \n",
    "& + p_1 (2 S - 1 - p_2 - 2 S p_2 ) + p_2 \n",
    "\\big] \\label{a}\\tag{1}\\\\\n",
    "= \\frac{1}{6} \\big[ \n",
    "& + p_2 (1 - p_1 - 2 S p_1) - p_1 + 2 S p_1\n",
    "\\big] \\label{b}\\tag{2}\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total utility for second player is $U_2 = -U_1$.\n",
    "\n",
    "From $(2)$, $1 - p_1 - 2 S p_1 \\gtrless 0 \\equiv \\frac{1}{1 + 2 S} \\gtrless p_1$\n",
    "\n",
    "**Case 1:** If $\\frac{1}{1 + 2 S} < p_1$,\n",
    "second player will pick $p_2 = 1$,\n",
    "and max utility for first player would be\n",
    "$\\frac{1}{6} [1 - 2 p_1] = \\frac{1}{6} \\frac{2 S - 1}{1 + 2 S}$\n",
    "\n",
    "**Case 2:** If $\\frac{1}{1 + 2 S} > p_1$,\n",
    "second player will pick $p_2 = 0$,\n",
    "and max utility for first player would be\n",
    "$\\frac{1}{6} p_1(2 S - 1) = \\frac{1}{6} \\frac{2 S - 1}{1 + 2 S}$\n",
    "\n",
    "**Case 3:** If $\\frac{1}{1 + 2 S} = p_1$,\n",
    "the utility will be independent of $p_2$,\n",
    "and max utility for first player would be\n",
    "$\\frac{1}{6} p_1(2 S - 1) = \\frac{1}{6} \\frac{2 S - 1}{1 + 2 S}$\n",
    "\n",
    "In all three cases the best strategy for first player is \n",
    "$p_1 = \\frac{1}{1 + 2 S}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly from $(1)$, best strategy for second player is $p_2 = \\frac{2 S - 1}{1 + 2 S}$\n",
    "\n",
    "At this point, neither player can increase their utilities by changing the strategies.\n",
    "Therefore it is the Nash equilibrium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try plotting these in charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'analytics' from 'labml' (/opt/anaconda3/lib/python3.12/site-packages/labml/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlabml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analytics\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlabml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger, experiment, monit, tracker\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlabml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Text\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'analytics' from 'labml' (/opt/anaconda3/lib/python3.12/site-packages/labml/__init__.py)"
     ]
    }
   ],
   "source": [
    "from typing import List, NewType, Dict, cast\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import torch\n",
    "from labml import analytics\n",
    "from labml import logger, experiment, monit, tracker\n",
    "from labml.logger import Text\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S$, `BLINDS`, is the number of chips each player needs to bet before seeing cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLINDS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets calculate $U_1$, `u_1` for all values of $p_1$ (`p_`) and $p_2$ (`p_2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m p_1, p_2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmeshgrid(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m5\u001b[39m), \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      2\u001b[0m p_1, p_2 \u001b[38;5;241m=\u001b[39m p_1 \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m, p_2 \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 3\u001b[0m p_1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(p_1, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m p_2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(p_2, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m u_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m6\u001b[39m \u001b[38;5;241m*\u001b[39m (p_2 \u001b[38;5;241m-\u001b[39m p_1 \u001b[38;5;241m-\u001b[39m p_1 \u001b[38;5;241m*\u001b[39m p_2 \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m BLINDS \u001b[38;5;241m*\u001b[39m p_1 \u001b[38;5;241m*\u001b[39m p_2 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m BLINDS \u001b[38;5;241m*\u001b[39m p_1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "p_1, p_2 = np.meshgrid(range(0, 100, 5), range(0, 100, 5))\n",
    "p_1, p_2 = p_1 / 100, p_2 / 100\n",
    "p_1 = torch.tensor(p_1, requires_grad=True)\n",
    "p_2 = torch.tensor(p_2, requires_grad=True)\n",
    "u_1 = 1/6 * (p_2 - p_1 - p_1 * p_2 - 2 * BLINDS * p_1 * p_2 + 2 * BLINDS * p_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix():\n",
    "    x = p_1.detach().numpy().ravel() # * 100\n",
    "    y = p_2.detach().numpy().ravel() # * 100\n",
    "    z = u_1.detach().numpy().ravel()\n",
    "\n",
    "    data = []\n",
    "    for i in range(len(x)):\n",
    "        data.append({\n",
    "            'p1': x[i],\n",
    "            'p2': y[i],\n",
    "            'u1': z[i]\n",
    "        })\n",
    "\n",
    "    source = alt.Data(values=data)\n",
    "\n",
    "    return alt.Chart(source).mark_rect().encode(\n",
    "        x='p1:N',\n",
    "        y=alt.Y('p2:N', sort='descending'),\n",
    "        color='u1:Q'\n",
    "    )\n",
    "\n",
    "plot_matrix()\n",
    "# Image('images/utility.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above chart shows $U_1$ against $(p_1, p_2)$.\n",
    "\n",
    "Lets plot the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grads():\n",
    "    u_1.sum().backward(retain_graph=True)\n",
    "    p_1_grad = p_1.grad.clone()\n",
    "    p_1.grad.zero_()\n",
    "    p_2.grad.zero_()\n",
    "\n",
    "    (-u_1).sum().backward(retain_graph=True)\n",
    "    p_2_grad = p_2.grad.clone()\n",
    "    p_1.grad.zero_()\n",
    "    p_2.grad.zero_()\n",
    "    \n",
    "    _, ax = plt.subplots(figsize=(9, 9))\n",
    "    quiver = ax.quiver(p_1.detach(), p_2.detach(), p_1_grad, p_2_grad)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nash equilibrium lies around $(0.33, 0.33)$ when $S = 1$.\n",
    "The gradients seem to circle around it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual Regret Minimization (CFR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counterfactual regret minimization use regret matching to find a strategy\n",
    "that minimize current regrets.\n",
    "The average strategy over a large number of iterations will converge to \n",
    "Nash equilibrium.\n",
    "\n",
    "[An Introduction to Counterfactual Regret Minimization](http://modelai.gettysburg.edu/2013/cfr/cfr.pdf)\n",
    "gives a detailed introduction.\n",
    "Proofs can be found on\n",
    "[Monte Carlo Sampling for Regret Minimization inExtensive Games](http://mlanctot.info/files/papers/nips09mccfr.pdf) and\n",
    "[Regret Minimization in Games with IncompleteInformation](http://martin.zinkevich.org/publications/regretpoker.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the code for running CFR on Kuhn Poker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$i$, `Player`, is the player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Player = NewType('Player', int)\n",
    "PLAYERS = cast(List[Player], [0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a$, `Action` is an action performed by a player or by chance.\n",
    "That is either a betting (`p` for pass and `b` for bet),\n",
    "or a card dealt (`A`, `K`, `Q`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Action = NewType('Action', str)\n",
    "ACTIONS = cast(List[Action], ['p', 'b'])\n",
    "CHANCES = cast(List[Action], ['A', 'K', 'Q'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h$, `History`, is the history of the current game.\n",
    "That is, the cards dealt and actions taken by the players.\n",
    "We represent $h$ is a string where the first two letters are the\n",
    "cards dealt to the two players and the rest are betting actions.\n",
    "\n",
    "Here are some example histories:\n",
    "\n",
    "1. `KA` - Player 1 gets *K*. Player 2 gets *A*. It is the Player 1's chance to take an action.\n",
    "2. `KAp` - Player 1 gets *K*. Player 2 gets *A*. Player 1 passes. The game ends. This is a terminal history.\n",
    "3. `KAb` - Player 1 gets *K*. Player 2 gets *A*. Player 1 bets a chip. It is the Player 2's chance to take an action.\n",
    "4. `KAbp` - Player 1 gets *K*. Player 2 gets *A*. Player 1 bets a chip. Player 2 passes. Game ends. This is a terminal history.\n",
    "5. `KAbb` - Player 1 gets *K*. Player 2 gets *A*. Player 1 bets a chip. Player 2 calls by betting a chip. Game ends. This is a terminal history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "History = NewType('History', str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$I$, InfoSet`, is an **information set**.\n",
    "This represents the information available to the player.\n",
    "This is like the state of the agent.\n",
    "In case of Kuhn poker, it's the card in their hand and the betting info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoSet:\n",
    "    regret: Dict[Action, float]\n",
    "    strategy: Dict[Action, float]\n",
    "    cumulative_strategy: Dict[Action, float]\n",
    "    average_strategy: Dict[Action, float]\n",
    "\n",
    "    def __init__(self, key: str):\n",
    "        self.key = key\n",
    "        self.regret = {a: 0 for a in actions(self)}\n",
    "\n",
    "        n_actions = len(actions(self))\n",
    "        self.strategy = {a: 1 / n_actions for a in actions(self)}\n",
    "        self.cumulative_strategy = {a: 0 for a in actions(self)}\n",
    "        self.average_strategy = {a: 0 for a in actions(self)}\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return repr(self.strategy)\n",
    "\n",
    "\n",
    "INFO_SETS: Dict[str, InfoSet] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`player` gives the current player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def player(h: History):\n",
    "    return cast(Player, len(h) % 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `info_set` gives the Information set $I$ for a given history $h$.\n",
    "\n",
    "Here are some example history & infoormation set pairs:\n",
    "\n",
    "* `KA` - Player 1 is the current player. He can see his card *K*. So the information set is `K`.\n",
    "* `KAb` - Player 2 is the current player. He can see his card *A* and the betting history *b*. So the information set is `Ab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_set(h: History) -> InfoSet:\n",
    "    i = player(h)\n",
    "    visible = h[i] + h[2:]\n",
    "    if visible not in INFO_SETS:\n",
    "        INFO_SETS[visible] = InfoSet(visible)\n",
    "\n",
    "    return INFO_SETS[visible]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A(I)$, `actions`, gives the set of actions the player\n",
    "can take at the current state represented by $I$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actions(I: InfoSet):\n",
    "    return ACTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `is_chance` checks if the next action is a chance action,\n",
    "and `sample_chance` samples an action at a chance action.\n",
    "This is used to deal cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chance(h: History) -> bool:\n",
    "    return len(h) < 2\n",
    "\n",
    "def sample_chance(h: History) -> Action:\n",
    "    while True:\n",
    "        r = np.random.randint(len(CHANCES))\n",
    "        chance = CHANCES[r]\n",
    "        for c in h:\n",
    "            if c == chance:\n",
    "                chance = None\n",
    "                break\n",
    "\n",
    "        if chance is not None:\n",
    "            return cast(Action, chance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma_i^t (I)$, `strategy`, is the strategy of player $i$ at iteration $t$ of training for\n",
    "information set $I$. It gives the probabilities for actions $a \\in A(I)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy(I: InfoSet):\n",
    "    return I.strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$u_i(z)$, `terminal_utility`, is the utility of player $i$ at terminal game history $z$.\n",
    "A terminal game history is history of a game that has completed.\n",
    "`is_terminal` returns if a given history is a terminal, and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_terminal(h: History):\n",
    "    if len(h) <= 2:\n",
    "        return False\n",
    "    if h[-1] == 'p':\n",
    "        return True\n",
    "    if h[-2:] == 'bb':\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def terminal_utility(h: History, i: Player) -> float:\n",
    "    winner = -1 + 2 * (h[0] < h[1])\n",
    "    utility = 0\n",
    "    if h[-2:] == 'bp':\n",
    "        utility = BLINDS\n",
    "    elif h[-2:] == 'bb':\n",
    "        utility = winner * (1 + BLINDS)\n",
    "    elif h[-1] == 'p':\n",
    "        utility = winner * BLINDS\n",
    "\n",
    "    if i == PLAYERS[0]:\n",
    "        return utility\n",
    "    else:\n",
    "        return -utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected utility at $h$ for player $i$ is, with with $\\sigma$ strategies of players is,\n",
    "\n",
    "$$u_i(\\sigma, h) = \\sum_{z \\in Z, h \\sqsubset z} \\pi^\\sigma (h, z) u_i(z)$$\n",
    "\n",
    "where,\n",
    "* $Z$ is the set of all terminal histories\n",
    "* $h \\sqsubset z$ means $h$ is a prefix of $z$\n",
    "* $\\pi^\\sigma (h, z)$ is the probability of reaching $z$ from $h$ with $\\sigma$ strategies of players.\n",
    "\n",
    "$u_i(\\sigma_{I \\to a}, h)$ is the utility if it always took action $a$ at current information set.\n",
    "\n",
    "We compute $u$ recursively,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "u_i(\\sigma_{I \\to a}, h) &= u_i(\\sigma, h a) \\\\\n",
    "u_i(\\sigma, h) &= \\sum_{a \\in A(I)} \\sigma(I, a) u_i(\\sigma_{I \\to a}, h) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Counterfactual value* at a non-terminal history h is,\n",
    "$$v_i(\\sigma, h) = \\pi_{-i}^\\sigma (h) u_i(\\sigma, h)$$\n",
    "\n",
    "where\n",
    "* $\\pi_{-i}^\\sigma (h)$ is the probability of reaching $h$ if player $i$ took all of its actions that lead to $h$ with a probability of $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Counterfactual regret* of not taking action $a$ at history $h$ is\n",
    "$$r_i(h,a) = v_i(\\sigma_{I \\to a}, h) - v_i(\\sigma, h)$$\n",
    "\n",
    "*Counterfactual regret* of not taking action $a$ at information set $I$ is\n",
    "$$r_i(I,a) = \\sum_{h \\in I} r_i(h, a)$$\n",
    "\n",
    "The cumulative counterfactual regret over all iterations up to $T$ is,\n",
    "$$R_i^T (I, a) = \\sum_{t=1}^T r_i^t (I, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep track of $R_i^T$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regret(I: InfoSet):\n",
    "    return I.regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`update` updates $R_i^T$ with counterfactural values.\n",
    "\n",
    "* `va[a]` is $v_i(\\sigma_{I \\to a}, h)$\n",
    "* `v` is $v_i(\\sigma, h)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_regrets(I: InfoSet, v: float, va: Dict[Action, float], i: Player):\n",
    "    for a in actions(I):\n",
    "        regret(I)[a] = regret(I)[a] + (va[a] - v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy is calculated with Hart and Mas-Collell's regret matching.\n",
    "$$\n",
    "\\begin{align}\n",
    "R_i^{T,+} (I, a) &= \\max (R_i^T (I, a), 0)\n",
    "\\\\\n",
    "\\sigma_i^{T + 1} (I, a) &= \\begin{cases}\n",
    "\\frac{R_i^{T,+} (I, a)}{\\sum_{a' \\in A(I)} R_i^{T,+} (I, a')},  &\\text{if $\\sum_{a' \\in A(I)} R_i^{T,+} (I, a') > 0$}\\\\\n",
    "\\frac{1}{|A(I)|}  & \\text{otherwise}\\\\\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_new_strategy(I: InfoSet):\n",
    "    r_plus = {a: max(r, 0) for a, r in regret(I).items()}\n",
    "    r_plus_sum = sum(r for r in r_plus.values())\n",
    "    n_actions = len(actions(I))\n",
    "    for a in actions(I):\n",
    "        if r_plus_sum > 0:\n",
    "            strategy(I)[a] = r_plus[a] / r_plus_sum\n",
    "        else:\n",
    "            strategy(I)[a] = 1 / n_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average strategy for player $i$ from iterations up to $T$ is,\n",
    "$$\\bar{\\sigma}_i^T (I, a) =\n",
    "\\frac{\\sum_{t=1}^T \\pi_i^{\\sigma^t} (I) \\sigma_i^T(I, a)}\n",
    "{\\sum_{t=1}^T \\pi_i^{\\sigma^t} (I)}$$\n",
    "\n",
    "where,\n",
    "* $\\pi_i^\\sigma(I) = \\sum_{h \\in I} \\pi_i^\\sigma(h)$\n",
    "* $\\pi_i^\\sigma(h)$is the probability of reaching $h$ if all players except $ð‘–$ took actions that lead to $h$ with a probability of $1$. This is equal to the sum of probabilities \n",
    "\n",
    "`update_cumulative_strategy` keeps the sum of $\\pi_i^\\sigma \\sigma_i^T(I, a)$ and\n",
    "`calculate_average_strategy` calculates $\\bar{\\sigma}_i^T (I, a)$.\n",
    "* `pi_i` is $\\pi_i^\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_cumulative_strategy(I: InfoSet, pi_i: float):\n",
    "    for a in actions(I):\n",
    "        I.cumulative_strategy[a] += pi_i * strategy(I)[a]\n",
    "        \n",
    "def calculate_average_strategy(I: InfoSet):\n",
    "    strategy_sum = sum(I.cumulative_strategy[a] for a in actions(I))\n",
    "    n_actions = len(actions(I))\n",
    "\n",
    "    for a in actions(I):\n",
    "        if strategy_sum > 0:\n",
    "            I.average_strategy[a] = I.cumulative_strategy[a] / strategy_sum\n",
    "        else:\n",
    "            I.average_strategy[a] = 1 / n_actions            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main recursive counterfactual regret minimization function.\n",
    "It recursevely explores the game tree and calculates a new stratgy.\n",
    "`cfr` returns $u_i(\\sigma, h)$.\n",
    "\n",
    "* `pi[i]` is $\\pi_i^\\sigma$\n",
    "* `ua[a]` is $u_i(\\sigma_{I \\to a}, h)$\n",
    "* `u` is $u_i(\\sigma, h)$\n",
    "* `va[a]` is $v_i(\\sigma_{I \\to a}, h)$\n",
    "* `u` is $v_i(\\sigma, h)$\n",
    "* `pi_neg_i` is $\\pi_{-i}^\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfr(h: History, i: Player, pi: Dict[Player, float]) -> float:\n",
    "    if is_terminal(h):\n",
    "        return terminal_utility(h, i)\n",
    "    elif is_chance(h):\n",
    "        a = sample_chance(h)\n",
    "        return cfr(h + a, i, pi)\n",
    "\n",
    "    I = info_set(h)\n",
    "    current_player = player(h)\n",
    "    \n",
    "    ua = {}\n",
    "    u = 0\n",
    "    for a in actions(I):\n",
    "        pi_next = pi.copy()\n",
    "        prob_action = strategy(I)[a]\n",
    "        pi_next[current_player] = pi[current_player] * prob_action\n",
    "        ua[a] = cfr(h + a, i, pi_next)\n",
    "        u += prob_action * ua[a]\n",
    "\n",
    "    if current_player == i:\n",
    "        pi_neg_i = 1\n",
    "        for j, pi_j in pi.items():\n",
    "            if j != i:\n",
    "                pi_neg_i *= pi_j\n",
    "\n",
    "        va = {a: pi_neg_i * ua[a] for a in actions(I)}\n",
    "        v = pi_neg_i * u\n",
    "\n",
    "        update_regrets(I, v, va, i)\n",
    "        update_cumulative_strategy(I, pi[i])\n",
    "        \n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate about $5,000$ iterations for illustrations. The average strategy settles completely after about $25,000$ iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve():\n",
    "    tracker.set_scalar('*', is_print=False)\n",
    "    for t in monit.loop(5_000):\n",
    "        for i in PLAYERS:\n",
    "            cfr('', i, {p: 1. for p in PLAYERS})\n",
    "        for I in INFO_SETS.values():\n",
    "            calculate_new_strategy(I)\n",
    "            calculate_average_strategy(I)\n",
    "            if (t + 1) % 10 == 0:\n",
    "                for a in actions(I):\n",
    "                    tracker.save({\n",
    "                        f'strategy.{I.key}.{a}': strategy(I)[a],\n",
    "                        f'average_strategy.{I.key}.{a}': I.average_strategy[a],\n",
    "                        f'regret.{I.key}.{a}': regret(I)[a]\n",
    "                    })\n",
    "    \n",
    "    logger.log('Average Strategy', Text.title)\n",
    "    logger.inspect({k: I.average_strategy for k, I in INFO_SETS.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create an experiment and start run CFR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "experiment.create(name='kuhn_poker', writers={'sqlite'})\n",
    "experiment.start()\n",
    "\n",
    "solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = analytics.runs(experiment.get_uuid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics.distribution(ind.average_strategy_A_b + ind.average_strategy_Ab_b,\n",
    "                      width=600, height=300)\n",
    "# Image('images/strategy_A.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see dominant strategies\n",
    "(when first player gets an A, when second player gets an A)\n",
    "have settled early on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics.distribution(ind.average_strategy_Q_b + ind.average_strategy_Kb_b + \n",
    "                       ind.strategy_Q_b + ind.strategy_Kb_b,\n",
    "                       width=600, height=300)\n",
    "# Image('images/strategy_QK.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the strategies for betting with a \n",
    "Q for first player and betting with a K for second player\n",
    "take time to converge.\n",
    "\n",
    "The average strategies $\\bar{\\sigma}_i^T (I, a)$ oscillate around Nash equilibrium while converging.\n",
    "The current strategy $\\sigma_i^T (I, a)$ goes in cycles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see them better on a scatter plots. Here $\\sigma_i^T(I, a)$ seems to be moving around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics.scatter(ind.strategy_Q_b, ind.strategy_Kb_b, noise=(0.02, 0.02),\n",
    "                  width=400, height=400)\n",
    "# Image('images/strategy_scatter.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst, average strategies $\\bar{\\sigma}_i^T(I, a)$ are spiraling into the Nash equilibrium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics.scatter(ind.average_strategy_Q_b, ind.average_strategy_Kb_b,\n",
    "                  width=400, height=400)\n",
    "# Image('images/avg_strategy_scatter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
